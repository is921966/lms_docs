name: Automated UI Tests with Smart Fixes

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

env:
  DEVELOPER_DIR: /Applications/Xcode_16.0.app/Contents/Developer
  SCHEME: LMS

jobs:
  ui-tests:
    name: Run UI Tests with Auto-Analysis
    runs-on: macos-15
    outputs:
      test-failed: ${{ steps.test.outcome == 'failure' }}
      failure-summary: ${{ steps.analyze.outputs.summary }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Xcode
        run: sudo xcode-select -s $DEVELOPER_DIR

      - name: Build for testing
        run: |
          cd LMS_App/LMS
          xcodebuild build-for-testing \
            -scheme LMS \
            -destination 'platform=iOS Simulator,name=iPhone 16,OS=18.0' \
            -derivedDataPath DerivedData

      - name: Run UI Tests
        id: test
        continue-on-error: true
        run: |
          cd LMS_App/LMS
          
          # Run tests with detailed logging
          xcodebuild test-without-building \
            -scheme LMS \
            -destination 'platform=iOS Simulator,name=iPhone 16,OS=18.0' \
            -derivedDataPath DerivedData \
            -resultBundlePath TestResults/UITests.xcresult \
            -test-timeouts-enabled YES \
            -maximum-test-execution-time-allowance 600 \
            -only-testing:LMSUITests \
            2>&1 | tee ui_test_output.log
          
          # Save exit code
          echo "exit_code=$?" >> $GITHUB_OUTPUT

      - name: Extract test results
        if: always()
        run: |
          cd LMS_App/LMS
          
          # Convert xcresult to JSON
          xcrun xcresulttool get \
            --path TestResults/UITests.xcresult \
            --format json \
            > test_results.json
          
          # Extract failure details
          python3 << 'EOF'
          import json
          import re
          
          with open('test_results.json', 'r') as f:
              data = json.load(f)
          
          failures = []
          for test in data.get('tests', {}).get('testNodes', []):
              if test.get('outcome') == 'failed':
                  failures.append({
                      'test': test.get('name'),
                      'error': test.get('failureMessage', '')
                  })
          
          with open('failures.json', 'w') as f:
              json.dump(failures, f, indent=2)
          EOF

      - name: Analyze failures
        id: analyze
        if: steps.test.outcome == 'failure'
        run: |
          cd LMS_App/LMS
          
          # Create detailed failure report
          echo "# UI Test Failures Report" > failure_report.md
          echo "" >> failure_report.md
          echo "## Failed Tests:" >> failure_report.md
          
          # Parse failures
          python3 << 'EOF' >> failure_report.md
          import json
          
          with open('failures.json', 'r') as f:
              failures = json.load(f)
          
          for failure in failures:
              print(f"\n### ‚ùå {failure['test']}")
              print(f"**Error**: {failure['error']}")
              
              # Suggest fixes based on error type
              error = failure['error'].lower()
              if 'no matches found' in error:
                  print("\n**ü§ñ AI Suggestion**: Update selector or add waitForExistence")
              elif 'timeout' in error:
                  print("\n**ü§ñ AI Suggestion**: Increase timeout or check element availability")
              elif 'nil' in error:
                  print("\n**ü§ñ AI Suggestion**: Add nil check or optional binding")
          EOF
          
          # Create summary for output
          SUMMARY=$(grep -c "### ‚ùå" failure_report.md || echo "0")
          echo "summary=$SUMMARY tests failed" >> $GITHUB_OUTPUT

      - name: Extract screenshots
        if: failure()
        run: |
          cd LMS_App/LMS
          mkdir -p screenshots
          
          # Extract screenshots from xcresult
          xcrun xcresulttool get \
            --path TestResults/UITests.xcresult \
            --output-path screenshots \
            --type screenshot || true

      - name: Create GitHub Issue
        if: steps.test.outcome == 'failure' && github.event_name == 'push'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cd LMS_App/LMS
          
          # Create issue with all details
          gh issue create \
            --title "ü§ñ UI Tests Failed - Auto-Fix Needed" \
            --body-file failure_report.md \
            --label "ui-tests,automated,ai-fix-needed" \
            --assignee "${{ github.actor }}"

      - name: Comment on PR
        if: steps.test.outcome == 'failure' && github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('LMS_App/LMS/failure_report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ü§ñ UI Tests Failed\n\n${report}\n\n[View full logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`
            });

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ui-test-results-${{ github.run_id }}
          path: |
            LMS_App/LMS/ui_test_output.log
            LMS_App/LMS/test_results.json
            LMS_App/LMS/failures.json
            LMS_App/LMS/failure_report.md
            LMS_App/LMS/TestResults/
            LMS_App/LMS/screenshots/
          retention-days: 30

      - name: Smart retry failed tests
        if: steps.test.outcome == 'failure'
        run: |
          cd LMS_App/LMS
          
          echo "üîÑ Attempting smart retry for failed tests..."
          
          # Extract failed test names
          python3 << 'EOF' > failed_tests.txt
          import json
          with open('failures.json', 'r') as f:
              failures = json.load(f)
          for failure in failures:
              print(failure['test'])
          EOF
          
          # Retry each failed test with increased timeout
          while IFS= read -r test; do
            echo "üîÑ Retrying: $test"
            xcodebuild test-without-building \
              -scheme LMS \
              -destination 'platform=iOS Simulator,name=iPhone 16,OS=18.0' \
              -derivedDataPath DerivedData \
              -only-testing:"LMSUITests/$test" \
              -test-timeouts-enabled YES \
              -maximum-test-execution-time-allowance 1200 \
              || echo "Still failing: $test"
          done < failed_tests.txt

  auto-fix-suggestion:
    name: Generate Auto-Fix PR
    runs-on: ubuntu-latest
    needs: ui-tests
    if: needs.ui-tests.outputs.test-failed == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: ui-test-results-${{ github.run_id }}
          
      - name: Generate fix suggestions
        run: |
          echo "# Automated Fix Suggestions" > fix_suggestions.md
          echo "" >> fix_suggestions.md
          echo "Based on the UI test failures, here are automated fix suggestions:" >> fix_suggestions.md
          echo "" >> fix_suggestions.md
          
          # Add specific suggestions based on failure patterns
          python3 << 'EOF' >> fix_suggestions.md
          import json
          
          with open('failures.json', 'r') as f:
              failures = json.load(f)
          
          for i, failure in enumerate(failures, 1):
              print(f"\n## {i}. Fix for {failure['test']}")
              error = failure['error'].lower()
              
              if 'no matches found' in error:
                  print("```swift")
                  print("// Add wait and use more flexible selector")
                  print("let element = app.buttons.matching(identifier: \"YourButton\").firstMatch")
                  print("XCTAssertTrue(element.waitForExistence(timeout: 10))")
                  print("element.tap()")
                  print("```")
              elif 'timeout' in error:
                  print("```swift")
                  print("// Increase timeout and add retry logic")
                  print("var retries = 3")
                  print("while retries > 0 && !element.exists {")
                  print("    sleep(2)")
                  print("    retries -= 1")
                  print("}")
                  print("XCTAssertTrue(element.exists)")
                  print("```")
          EOF

      - name: Post analysis summary
        uses: actions/github-script@v6
        with:
          script: |
            const summary = '${{ needs.ui-tests.outputs.failure-summary }}';
            
            core.summary
              .addHeading('UI Test Results')
              .addRaw(`‚ùå ${summary}`)
              .addLink('View detailed report', `https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}`)
              .write(); 